%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------

\documentclass[preprint,12pt]{elsarticle}

\usepackage{amssymb}
\journal{Computers and Operations Research}

\begin{document}

\begin{frontmatter}

\title{A study of exhaustive solutions for the MasterMind puzzle}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{}

% Antonio - No es doble revisión ciega, al menos el de CHAC no lo fue

\address{}

\begin{abstract}
Mastermind is in essence a search problem in which a string of symbols must be
found aided by hints that indicate how close are other strings to that
symbol. Although has been commercialized as a game, it is a combinatorial 
problem of high complexity, with applications on fields that range
from computer security to genomics. As such a problem, there are no
exact solutions; even exhaustive search methods rely on heuristics to
choose, at every step, strings to get the best possible hint. These
methods mostly try to play the move that offers the best reduction in
search space size. However, in this paper we will examine several
search strategies and show that another factor, the presence of the
% Antonio - otro término mejor que 'search strategies'... es que se repite mucho 'search'
solution among the candidate moves, plays also a very important
role. Using that, we will propose new exhaustive search approaches 
that obtain better results, and besides, are more suitable as a basis
% Antonio - ¿mejores resultados en qué sentido?
for non-exhaustive search strategies such as evolutionary algorithms. 
\end{abstract}

\begin{keyword}
MasterMind, bulls and cows, logic, puzzles, games, exhaustive search
% Antonio - algún otro término de búsqueda u optimización...
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{s:intro}

\section{Mastermind as a Puzzle}
\label{s:mm}

\section{State of the art}
\label{s:soa}

\section{Exhaustive search methods and how they work}
\label{s:sm}
% Antonio - lo de 'how they work' no se suena muy serio... ¿'Principles of Exhaustive search methods'?

% Antonio - Está claro que esta frase no es la primera. :D
Let us look first at the smallest usual size, $\kappa=6$ and
$\ell=4$. The best two strategies in this case have been proved 
to be Entropy and Most Parts \cite{nicso,Berghman20091880}, in both cases
starting using Knuth's rule, ABCA \cite{Knuth}. Number of game results
% Antonio - qué significa 'number of game results'? no sería 'number of moves'?
% Además hay media y máximo. ;)
are shown in Table \ref{tab:me}.
\begin{table}[t]
\caption{Average and Maximum number of moves for two search strategies: Most Parts
  and Entropy. \label{tab:me}}
  \centering
\begin{tabular}{|l|c|c|}
\hline
\emph{Method} & \emph{Number of moves: Average} & \emph{Number of
  moves: Maximum} \\
\hline
Entropy & 4.413 & 6 \\
Most Parts & 4.406 & 7 \\
\hline
\end{tabular}
\end{table}
%
To compute this average, 10 runs over the whole combination space were
made. In fact the difference in the number of moves is small enough to not
be statistically significant (using Wilcoxon test), 
% Antonio - ...enough not being statistically significant...?
but there is a
difference among them, the most striking being that, even if the
maximum number of moves is higher for Most Parts, Entropy has a higher
average. 

To check where that difference stems from we will plot a
histogram of the number of moves needed to find the solution for both
methods, see Fig. \ref{fig:histo:me}. 

\begin{figure}[!htbp]
\centering
\includegraphics{histo-me.eps}
\caption{Histogram with the count of the number of times every method
  is able to find the solution in every number of moves, for the
  Entropy method (black and solid) and Most Parts (light or red dashed). \label{fig:histo:me}}
\end{figure} 
% Antonio - IMPORTANTE: si has dicho en la tabla 1 que el máximo de movimientos para la Entropía es 6, ¿por qué llega
%                       la línea negra sólida al 7?
% Otras cosillas:
% quita el título del gráfico y pon la leyenda del eje X en solo una línea (o quita la palabra histogram)
% count es un poco genérico y no se sabe a qué se refiere, ¿no sería mejor 'games' o 'played combinations'?
% quizá podrías poner una leyenda dentro del gráfico con la descripción de las líneas
%

As should be expected for the negligible difference in the average number of moves, differences here
are very small. It is noticeable, however, than Most Parts finishes
less times in 1 to 3 moves, and also 5 moves. Most parts only finishes
in less occasions than Entropy for 4 moves; quite obviously, too,
there are a few times in which Most Parts needs 7 moves, but only
8 out of the total 12960. 
% Antonio - ¿12960 games?
These differences are small enough to not
be statistically significant, 
% Antonio - esto está calcado de una frase anterior. ;)
but indicate that there is a small difference in them, so we will try to seek what is its source;
these differences must be the cause why eventually Most Parts achieves
% Antonio - yo pondría 'Most Parts' y 'Entrophy' en cursiva para diferenciarlos mejor
a slightly higher average in moves.

Since both methods try to reduce the size of the consistent set, we
will look at their size and how it changes with the number of
moves. We will log the size of the remaining consistent combinations
at every step, and this is shown in Fig. \ref{fig:cset:me} and Table
\ref{tab:cset:me}. 


\begin{table}
\centering
\caption{Average and standard deviation of the number of combinations
  remaining after every move (or step). The numbers are the same after
  the  first move (not shown here), since they are playing the same one.\label{tab:cset:me}}
\begin{tabular}{|c|c|c|}
\hline
\emph{After move \#} & \emph{Entropy} & \emph{Most Parts} \\
\hline
2 & 23 $\pm$ 14& 24 $\pm$  15\\
3 & 3.1 $\pm$ 1.8 & 3.4 $\pm$ 2.4 \\
4 & 1.13 $\pm$ 0.35 & 1.17 $\pm$ 0.43 \\ 
5 & 1 & 1.02 $\pm$ 0.17 \\
6 & & 1 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!htbp]
\centering
\includegraphics{cset-me.eps}
\caption{Average number of combinations (or moves) remaining after
  each move, for the
  Entropy method (black and solid) and Most Parts (light or
  red dashed). Plot of the values in Table
  \ref{tab:cset:me}.\label{fig:cset:me}}
\end{figure} 
% Antonio - no sería un histograma, ¿no? 
% La línea negra creo que debería llegar hasta el 6, pero tomando el valor 0 en ese movimiento
% Yo además quitaría el título
%
In effect, the Entropy method is more efficient in
reducing the size of the set of remaining solutions. At every step,
the difference is significant (using Wilcox test). This difference is
of a single combination at the beginning, and it is reduced with time while
still being significant; however, this reduction at the first stages
of the game makes the search simpler and would, in principle, imply an
easy victory for the Entropy technique, as would be expected. However, the
result is a statistical tie, 
% Antonio - ¿qué significa 'lazo/corbata estadístico/a'?
with a slight advantage for the other
method, Most Parts. Besides, this reduction does not explain the
advantage found in Fig. \ref{fig:histo:me}: if the number of
solutions that remain is bigger (on average), why is Most Parts able
to find the solution at those same stages more often than Entropy,
which seems to be the key to success?

To find this out, we will look at another result. As explained above,
all methods are based on scoring consistent combinations according to
the partitioning of the set they yield, and then playing randomly one
of the combinations with the top score. If, by chance, the winning
combination is in that set of top scorers, there is a non-zero
possibility of playing it as next move and thus winning the match. We
will then look at whether the winning combination effectively is or
not among the top scorers at each step. Results are shown in Fig. 
\ref{fig:top:me} and Table \ref{tab:top:me}.

\begin{table}
\centering
\caption{Percentage of times the secret code is among the top scorers for
  each method.\label{tab:top:me}}
\begin{tabular}{|c|c|c|}
\hline
\emph{In move \#} & \emph{Entropy} & \emph{Most Parts} \\
\hline
2 & 0.1142857  & 0.3644788\\
3 & 0.2919495 & 0.5270987 \\
4 & 0.7721438 & 0.8242563 \\ 
5 & 0.9834515 &  0.9810066\\
6 & & 1 \\
\hline
\end{tabular}
\end{table}
%
\begin{figure}[!htb]
\centering
\includegraphics{top-me.eps}
\caption{Chance of finding the secret code among the top scorers for the
  Entropy method (black and solid) and Most Parts (light or
  red dashed). Plot of the values in Table
  \ref{tab:top:me}.\label{fig:top:me}}
\end{figure} 
% Antonio - ¿Por qué no llegan las líneas hasta el movimiento 6 como en la tabla?
% Además pondría una claración de qué son los scorers (al menos un par de palabras entre paréntesis)
% Como siempre, quitaría el título

Table \ref{tab:top:me} clearly shows that the probability of finding
the hidden combination among the top scorers increases with time, so
that in the 5th move is practically one. But it also shows that (in the first moves) there
is almost double the chance of having the winning combination among the top
ones for Most Parts than for Entropy, so, effectively, and obviously
depending on the size of the consistent set found at Table 
\ref{tab:cset:me}, the likelihood of playing the winning combination
is higher for Most Parts and its key to success. Making a back of the
envelope calculation, when the game arrives at the second move, which
% Antonio - which -> with?
roughly 11000 games do, 
% Antonio - do -> done?
a third of them will include the winning
combination among the top scorers, which again is roughly three
thousand. 
% Antonio - ¿por qué unas veces con número y otras con letra?
Being the size of the top scorers bounded by the consistent
set size (on average), one in 25 will draw it by pure chance, which is
around 100 occasions. This again will be three times as much as for
Entropy, accounting for a rough difference of 60 games. Effectively,
Entropy finishes on the second move 118 times, vs. 147 times Most
Parts, 
% Antonio - Most Parts do?
which is roughly on the same ballpark we have
% Antonio - ballpark???
computed. Anyway, this is just a back of the envelope calculation,
with consistent set size giving just an upper bound; the chance of
drawing the secret code (or any other combination, for that matter)
will depend on the actual size of the top scorers set; however, there
is such a big difference in the chance of the secret coding being in
that set that the actual size does not matter so much (in this case). 

This, in turn, implies one of the main results of this paper: 
% Antonio - más que 'results' yo diría 'conclussions'?
there are two factors in the success of a method for playing Mastermind. The
first is the reduction it achieves on the search space size by playing
combinations that reduce it maximally, but there is a second and
non-negligible factor: the chance of playing the winning
combination by having it among the top scorers. 

This chance will decrease when the problem size is increased, so we
will perform the same measurements for problem size $\kappa=8, \ell=4$
and $\kappa=6,\ell=5$. Search space size is 4 times as big for the
former, around 8 times for the latter; time to solution grows faster
than lineally so it is not practical (although possible) to work with
exhaustive search for sizes bigger than that. While the exhaustive
solution to the smallest Mastermind size considered takes around half
a second, it takes around 5 seconds for $\kappa=8, \ell=4$, and 10
seconds for $\kappa=6,\ell=5$. In practice, this means that instead of
using the whole search space 10 times over to compute this average, we will generate a
particular set of 5000 combinations. This instance set is available at
the method website.
% Antonio - poner url en nota al pie si acaso
A solution is generated, then, for every one of
these combinations (some of which, of course, might be repeated, since
the search space is a bit smaller). 

The average number of moves is represented in
Table \ref{tab:me:46}. This confirms in parts our above hypothesis:
the better capability Entropy has to decrease the size of the search
space gives it an advantage in the average number of moves, keeping at the
same time the ability of solving it in less maximum number of
moves. However, for this number of experiments, the difference is only
significant at the 85\% level (Wilcoxon paired test = 0.1256). We will check
% Antonio - 'a continuación' 
whether this (small) difference has the same origin as we hypothesized
for the smaller search space size. 


\begin{table}[t]
\caption{Average and Maximum number of moves for two search strategies: Most Parts
  and Entropy for $\kappa=8, \ell=4$. \label{tab:me:46}}
  \centering
\begin{tabular}{|l|c|c|}
\hline
\emph{Method} & \emph{Number of moves: Average} & \emph{Number of
  moves: Maximum} \\
\hline
Entropy & 5.149 & 8 \\
Most Parts & 5.174 & 9 \\
\hline
\end{tabular}
\end{table}
%
\begin{figure}[!htb]
\centering
\includegraphics{histo-me-4-8.eps}
\caption{Frequency of the number of moves up to (and including) the
  secret combination, for $\kappa=8, \ell=4$. As usual, red or light dashed line 
  represents Most Parts and solid black line Entropy. \label{fig:histo:me:46}}
\end{figure} 
%
% Antonio - quitaría el título, lo de 'histrograma' y cambaría 'count'

The equivalent to Fig. \ref{fig:histo:me} has been represented in Fig \ref{fig:histo:me:46}.
The shape is similar, but the solid line that represents Entropy is below the
dashed line for Most Parts for most number of moves, and most
specially at the end, which accounts for the small difference in
average number of moves. Is this difference accounted for by the size
of the search space after each move? As previously, we will plot it in
Fig. \ref{fig:cset:me:46} and Table \ref{fig:cset:me:46}. 
Differences for all moves are significant according to Wilcoxon test.

\begin{table}
\centering
\caption{Average and standard deviation of the number of combinations
  remaining after every move. The number of combinations is the same after
  the  first move (not shown here), since both of them are playing the same move.
  \label{tab:cset:me:46}}
\begin{tabular}{|c|c|c|}
\hline
\emph{After move \#} & \emph{Entropy} & \emph{Most Parts} \\
\hline
2 & 98 $\pm$ 67 & 102 $\pm$ 69 \\
3 & 13 $\pm$ 10 & 14 $\pm$ 12 \\ 
4 & 2.4 $\pm$ 1.6 & 2.63 $\pm$ 1.96 \\
5 & 1.21 $\pm$ 0.46 & 1.27 $\pm$ 0.52\\
6 & 1.07 $\pm$ 0.27 & 1.08 $\pm$ 0.31 \\
7 & 1 & 1.25 $\pm$ 0.46 \\
\hline
\end{tabular}
\end{table}
%
\begin{figure}[!htb]
\centering
\includegraphics{csets-me-4-8.eps}
\caption{Average size of the consistent set, that is, the set of
  solutions that have not been discarded at a point in the game for
  $\kappa=8, \ell=4$. As usual, red or light dashed line 
  represents Most Parts and solid black Entropy. \label{fig:cset:me:46}}
\end{figure} 
% Antonio - lo mismo de antes

It should be expected that the probability of finding the secret code
among the top scorers will change; since the number of elements in the
consistent set intuitively we should expect it to decrease. But we are
wrong, as shown in Table \ref{tab:top:me:48} and Fig. \ref{fig:top:me:48}. 
In fact, if we compare these results with
those shown in Table \ref{tab:top:me} we see that, for the same move,
the proportion of times in which the secret code is among the top
scorers is almost twice as big at the beginning for Most Parts and
almost three times as big for Entropy. Interestingly enough, this also
implies that, while for $\kappa=6, \ell=4$ this probability was three
times as high for Most Parts, it is only two times as high now. 


\begin{table}
\centering
\caption{Percentage of times the secret code is among the top scorers for
  each method, $\kappa=8, \ell=4$.\label{tab:top:me:48}}
\begin{tabular}{|c|c|c|}
\hline
\emph{In move \#} & \emph{Entropy} & \emph{Most Parts} \\
\hline
2 & 0.3008602  & 0.6379276 \\
3 & 0.3425758 & 0.6957395 \\
4 & 0.4937768 & 0.7845089 \\ 
5 & 0.8630084 & 0.9160276\\
6 & 0.9898990 & 1 \\
\hline
\end{tabular}
\end{table}
%
\begin{figure}[!htb]
\centering
\includegraphics{top-me-4-8.eps}
\caption{Chance of finding the secret code among the top scorers for the
  Entropy method (black and solid) and Most Parts (light or
  red). This plot corresponds to the numbers represented in table
  \ref{tab:top:me:48} for $\kappa=8, \ell=4$.\label{fig:top:me:48}}
\end{figure} 
% Antonio - Ojo con la ref de la tabla (la he actualizado). 
% El título está mal y lo quitaría

This decrease in the chance of finding the secret code might
explain the difference in the average number of moves needed to find
the solution, which for this size tilts the balance in the direction
of Entropy. While for the smaller size this probability was enough to
compensate the superior capability of the Entropy method in reducing
the size of the search space, in this case the difference is not so
high, which makes the Entropy method find the solution, on average, on
less moves.

In turn, this implies that a method for efficiently finding the
solution to mastermind must combine both factors profitably to beath
either Most Parts or Entropy. We will propose such methods in the next
section. 

% Antonio - Yo pondría una tablita (o párrafo) de resumen diciendo las ventajas e inconvenientes de cada método o 
% poniendo quien gana en cada caso, por ejemplo. Así quedaría todo más claro, ya que yo me he hecho un pequeño lío
% y ahora mismo no se qué método funciona mejor ni exáctamente por qué. T,abién puede ser cosa del sueño. XD

\section{New exhaustive search methods}
\label{s:nsm}

\section{Conclusions and discussion}

\bibliographystyle{elsarticle-num}
\bibliography{geneura,mastermind}

\end{document}

